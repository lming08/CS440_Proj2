\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{graphicx}
\usepackage{graphicx}

\begin{document}

\title{CS 440: Project 2\\ A Document Classifier\\ Due
  April 26th}
\author{Gabe Dunham, Jeff Milner, Ryan Quackenbush}

\maketitle

\section{Abstract}
A brief summarization the paper in a single paragraph.
This documents aims to discuss the strategies our group took in implementing a basic document classifier between three document types: Deed of Reconveyance, Deed of Trust, and Liens. We will use three different strategies and to parse and determine a type for each document in the test files given.

\section{Introduction}
Given we have three different document types: Deeds of Reconveyance, Deed of Trust, and Liens, we are to parse through all the test data and be able to determine what type of documents they are. We are given three strategies: intelligrep, naive bayes, and perceptrons to evaluate and test the documents on through training, if necessary, and testing. The results will then be used to determine the percentage of the test data which is classified correct. The data provided in for each classification will be used to train the  naive bayes and perceptrons. A TEST data directory will contain all necessary files to run our tests and compare against a results file to display the percentage of documents classified correctly.
The 'Intelligrep' strategy was implemented and improved by Ryan Quackenbush.
The Naive Bayes strategy was implemented and improved by Jeff Milner.
The Perceptron strategy was implemented and improved by Gabriel Dunham.

Those were the main roles we assigned, but we each helped out all over the place. For exmple, Ryan implemented a few hefty features in the Naive Bayes improvement, Jeff helped improve Intelligrep, while Gabriel helped with the initial intelligrep strategy and improving the attribute sets. The attributes sets, file input, and file cleaning, were all originally implemented Jeff and Ryan.

\section{Obtaining the Feature Sets}
In order to use naive bayes, it was necessary to determine a set of attributes which were common
to each class of files. This was also used for training the perceptrons in the perceptron strategy. The base line version implementation
obtained this list of attributes by doing the following.
1: Counting the number of times that each word appeared in each document in the current class.
2: Dividing that number by the total number of words in C.
3: Taking the twenty words with the largest corresponding number.
In order to save time on computation, and because the number of words in C doesn't change for each word in C,
we ended up skipping the second set.

It should be noted that we found this worked pretty poorly for the perceptrons and naive bayes, so one improvement to our strategy was to change how the attribute sets are obtained. More on that later though.

\section{Initial Strategies}
\subsection{IntelliGrep Strategy}
The initial strategy was to classify each document on whether a certain word / phrase appeared most often in the document. The words used were "deeds of reconveyance", "deeds of trust", and "lien" for Deeds of Reconveyance, Deeds of Trust, and Liens respectively. This strategy on our test data came out with values around 15 percent accuracy overall. Initially this was due to attempting to run the algorithm on the training data. Switching our data around to work on the test data improved our accuracy to be in the 60 percent ball park since randomizing the tie breakers varied.

\subsection{Naive Bayes}
The inital implementation of Naive Bayes worked by using the intial feature list to find the words that were most common to all the files. Once this feature set had been decided for each class, the probability of finding that word in any document from a given class was calculated. This probably was calculated by counting the number of documents containing the word and divided by the total number of documents.

Testing a document was accomplished by giving each class an initial probability of one. Then, for each word in the feature list of the current class of the document, the starting probability is multiplied by the probability of that word being in the document, but only if that word is in the current document. If it is not, then the probability is multipled by the probability of that word not being in the document. After the probabilities are calculated, the document class with the highest probability of being correct is chosen.

The base strategy for Naive Bayes worked out reasonably well to start. The bag of words was initially not boolean, therefore causing issues with the output not being correct. Once we had switched it to use the boolean bag of words, the program started working closer to how we had expected it to, getting around 83 percent. Another problem we had was that when transferring the files, the indentation seemed to screw up and indent things weird so we had to go fix those, which improved it to having about 93 percent correctness after training and checking it against the given results file. We cleaned the files and compared the words from the files against a list of common word that we found to pop up often, as well as some others just in case they do pop up. At times, adding or removing a word from the list would increase the percentage by a few points.


\subsection{Perceptron}
The perceptron strategy is fairly simple. A perceptron takes several inputs, does some calculations based on those inputs, and then provides an output. It can, in some respects, be viewed as a function. In this implementation, we use a predetermined attribute set as the inputs. Each input's value is determined by counting the number of times the word appears in the document and then dividing by the total number of words in the document. If the word doesn't appear at all, we use one divided by the total number of words in the document. Each input is multiplied by a weight and the sum is taken over all the products. That is, the i-th input is multiplied by the i-th weight and then added to the running total until there are no more to multiple. If the the value of this sum of products is greater than the given threshold, the perceptron votes that the document is the type that it has been trained to recognize. Otherwise, it votes that the document is not the type which the perceptron has been trained to recognize.

The perceptron is trained on a given set of files. The file type is know, but the perceptron is allowed to make a guess before it is 'told' that it is right or wrong. Learning takes place whenever a perceptron guesses incorrectly; Either because it guessed that it was the type it is being trained to recognize when the file isn't, or because it guessed that the file was not the type that the perceptron is being trained to recognize even though the file was.
In this implementation, we used three perceptrons. One for each type of document. The perceptrons were each trained on all document types, as per the specification. One thing which is worth noting is that we use a for loop which looked something like this when training the perceptrons.
For each type of document in the training documents, do
	For each document in this type of training document, do
		Train the perceptrons on this document.
This will come into play later when we improve this strategy.
 
\section{Improved Strategies}
\subsection{IntelliGrep Strategy}
The slight improvement for intelligrep was to change our search terms to only be upper camel case or all upper case. The reasoning behind switching is for the way document titles, headers, or section names generally lay out according to the English language. Since most titles and headers represent the content of the document as an overview, if changing the search terms resulted in matches against the titles and header we might find that our searches are far more accurate. Our implementation was simply changing the search terms to "DEEDS OF TRUST", "DEED OF RECONVEYANCE", and "LIEN". Inside of the search loop we added any instances where the exact wording, including case, matched the term and added any case where the full capitalized term was found in the document.

The results ended up being 13 to 15 percent higher to the original base strategy, but with the random factor being a few percentage points on both sides of the low / high end. The extra percentage points on both sides simply is a result of more ties between the text, likely because there are fewer titles, or capital words, to match against. As far as the percent increase, our initial hypothesis proved to work out, the titles or headers were a good representation of the document as a whole. Another improvement may be to check the location of the titles in relation the the top, and given a weight based on that location. The purpose would be to give more weight to a title closer to the top because higher up title may be even more relevant than any given title or header throughout the document.

\section{Obtaining the Feature Sets}
In our original implementation we noticed something strange happening in the results of our naive bayes and perceptron strategies. In particular we noticed that a lot of the feature vectors in the documents we were looking at had no occurrences of the words that were in that document classes feature set. This seemed to be caused by the fact that some documents were much longer than others. We noticed that documents tend to repeat the same words frequently,  and if a document, which is much longer than the rest, repeats the same words over and over, then it can cause the feature set for that type of document to be very unrepresentative of the document type. This caused problems for both the Naive Bayes implementation as well as the Perceptron implementation.

Naive Bayes, because of the way it uses the differences between types of documents, ended up having very little useful information that it could use in determining the type of a document that it was looking at. It would often give low percentage guesses for all three document types when classifying a given document. The matter was worsened by the fact that these words were often common to all three document types. Actually, it was mostly due to the latter reason.
Perceptrons also seemed to be performing poorly as a result of similar reasons. They were being trained on results which didn't help them distinguish between the different types of documents.

In order to get better results for our Naive Bayes and Perceptrons implementations, we thought that it might help to find words which were more "common" to all the files in a particular file type. For example, if every document in Lien contains the word Lien, then that word should be heavily weighted. Instead of giving a single word a heavy weight because it showed up in a single document two hundred times. We also thought that it might be a good idea to get rid of some words which showed up in every document type, such as "he", "she", ect. Finally we didn't allow any words which consist of a single letter.
We accomplished these things by doing the following for each word in each document in the given document class.
If we've seen the word in this document, don't do anything with it.
If this is the first time we've seen this word in the document, add one to this class's counter for that word and add it to the list 
of words we've seen in this document.
If we've never the word in this class of document before, and if the word isn't a really common word, then add it to our class's word counter
list and to the list of words we've seen in this document.

This improvement helped Naive Bayes jump from an average accuracy of 76.4% to 92.4%. It was really surprising to see the accuracy drop on the perceptrons though. Just changing from counting the appearance of each word in all documents to only counting each word once per document had almost no effect on perceptrons. Changing the attributes to leave out the common words, however, caused the average accuracy of the perceptron strategy to drop from 86.22% to 72.44%! We suspect that this was because the perceptrons gathered information on the frequency of the word within each document instead of looking at whether or not the word occurred in the document. So, even if a word is common to all three types of documents, the frequency within each type of document was useful information for the preceptrons.

Although we didn't have time to investigate it, we suspect that for Naive Bayes, it would be very beneficial to only allow words to be in an attribute set for a document type, if it is not in an attribute for any other document type. It may be an over simplification of the problem, but we would have like to test it if we had had the time.


\subsection{Naive Bayes}
The original naive bayes seemed to lack  the ability to account for multiple instances of the "top words" we retrieved in our feature sets. This initially required some changing of how the feature sets are calculated and counted towards the "score" of each classifcation while looping through the feature set. 

The initial strategy involved using the boolean bag of words which would essentially see if the word merely existed in the document and add a weight based on it's presence or non-presence. This could lead to documents which are in considered for a classification based on one word. We started thinking about how we could incorporate a strategy which took into account the increased chance with certain words. This would require reclassifying our top words from the attribute sets to include not only the possibility of the word being in the document for a given classification, but also how many times in the classifcation as whole the word appeared in comparison the the total amount of words and unique words. If the word appeared more for a certain classification over the total amount of the words, it would make sense to give the word more weight, which we did having the "score" it contributed be a total of the number of times it appeared over the total times in the classifcation it appeared and the total number of unique words. 

The results ended up being about the same as the non-improved strategy staying at a constant 92 percent. We believe there may have been a few reasons for this. One reason would be the words read in were more even distributed among the documents, since the classifications all were a type of legal document. Another is the feature set we originally generated was a finite percentage of each classification's words. If we calculated the value of every word's impact on the document with exception of the most common, we may have seen better results since each word plays as much a negative effect as a positive effect similiar to how the boolean bag of words contributed.

\subsection{Perceptron}
One thing which we noticed in implementing the initial perceptrons was that the order of training didn't change because the perceptrons were always trained on all of the documents in a given document class at a time before being trained on any documents in any of the other document classes. We suspected that this may have been contributing to the fairly low accuracy. In order to increase the accuracy, we changed the order in which the perceptrons were trained on the documents. We did this at the same time that we implemented the ten-fold cross validation. So, we take all the files and divide them into ten groups. We ensure that each of the ten groups has about the same number of documents, and that each type of document shows up about the same number of times in each of the ten groups.
This increased the accuracy from an average of 86 to about 90, with some outliers in the 95% range.

One thing we noticed, but didn't have time to test, was that certain large files had things in common with other large files of the same type. That is, files which had around ten thousand words seemed to have word frequencies that were similar to other large files. We think the same may have been true for files with only a couple hundred words. So one idea, which we didn't have time to test, would have been to train several perceptrons for each class of documents, but have each one cover different sizes of documents. For example, the first perceptron is trained on files smaller than 300 words. The second one is trained on files that are larger than 300 words, but smaller than 600 words, and so on. Then, when classifying a new document, have all the perceptrons vote on the new document, but allow perceptrons who are closer in size to have a greater vote on whether or not they think it is their type of document. In this way, the training data that had information which was more likely to help identify the given document has a higher vote. 

\section{Conclusion}
In closing, we found that intelligrep was not the best. Not even when we attempted to improve it. The title of best strategy seems to be held in close contention between the improved Naive Bayes, which holds a steady 92 percent on the non-cross fold validated data, and the perceptron strategy, which hold an average of about 90% on cross fold validated tests. It seems like Naive Bayes may just be lucky with the given test data, but even so it is still a higher percentage and so we expect it to perform the best on the hidden test data. The Naive Bayes implementation, with only a few common words being removed, seems to do really well. This is a strong feature for it because it allows it to better differentiate between document classes and it chooses the maximum value of the three percentages instead of performing a binary vote and randomly choosing one of the yes votes. So for those reasons, as well as the slightly higher percentage, we choose Naive Bayes as our champion strategy.

\section{Future Courses}
\subsection{Gabe D}
Most useful thing you learned in class?
I learned that genetic algorithms are more "creative" than other search methods, but they're also much slower. I thought I'd had a tight grasp on just how slow they were before, but I had no idea.

If there was more time, I would have liked to learn about how to create learning agents that operate in an unkown environment. I think that is probably one of the harder things to teach though, so maybe it's not fair to say that. I haven't seen any good ways of implementing one yet, but I think that genetic algorithms are probably one of the better solutions for doing just that. 

To make more time, I would have preferred to see less about graph search. I mean, it's good stuff, but I feel like it's something that I could teach myself by looking up examples on the internet somewhere. I'm more interested in getting to the newer/harder stuff.

In general I liked the ideas that were in the group project. I liked playing with algorithms to try and speed them up or increase their accuracy. I didn't like have to write the massive papers on the subjects though, and I always felt like I was way too rushed to finish them. It was as though I'd been allowed to become curious on the subject of each project, but only just in time to be yanked away from it. I also felt unconfident writing research papers on stuff that I'd just learned. It also seems like, if we don't have much time to play with the research, then writing a five page paper is too much. 

Did I find these exercises valuable? Yes and no. I think that I definitely learned quite a bit and that I had an opportunity to become curious about some of the subjects. But I don't think that the writing excercises prepared me for writing research papers/project reports. For the research, this is because the projects are laid in stone and I'd probably have put on my unique-thoughts-hat for an actual research project. It also wasn't a project report, or at least not in the weekly-project-report-you'd-give-to-your-boss kind of way.
Do you plan on attending graduate school? I haven't decided yet. A year ago, I would have said no though.

Did the group projects change your interest in persuing graduate study? Eh?.. no. I might investigate a few ideas I had on creating a better A.I. though. They were the kind of ideas that I was hoping to get by taking this class, so props to you on that. I have to say that the group projects were much more difficult than the routine homework, and I think that it might have been helpful to make them more bite sized, but I don't know that for sure. They were also more interesting than the routine homework because I had a chance to see multiple algorithms solving the same problems within the same assignment. Normally I only ever see different algorithms solve different problems over the course of a few weeks. 

\subsection{Jeff M}

\subsection{Ryan Q}
Most useful thing you learned in this class? 
The most useful thing I learned was the search algorithms earlier in the course and more specifically A* search.

Finish the sentence: If there was more time, I would have liked to learn more about multi-agent evironments.

Finish the sentence: To make more time, I would have preferred to see less about... NOTHING! I felt the course subjects were relevant and liked the overall layout.

Group Projects likes / dislikes: I liked the group work, being able to choose our language if desired, spending a little more time on an assignment, and seeing results from what we learned. I disliked the length / interference of the paper, and the choice of topics we had to stick too. Having a little more choice would be great.

Writing, experiments, and analysis of projects? I find the experimenting, and analysis portion of the project valuable as it allows us to see practically what we thought would or wouldn't happen. I don't feel the writing really helped bring out the experimentation / analysis portion of the experiment, but more or less just took time away from it. I do not plan on attending graduate school immediately after undergrad, but may in the future. 

Project change interest? The projects had no affect on my thought of going to graduate school later. The overally class changed how interesting I thought AI would be, but the projects didn't specifically change my mind on anything. 


\end{document}
